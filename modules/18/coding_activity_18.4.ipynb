{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "984f8e1592d945984272a3b8f0fe7f7e",
     "grade": false,
     "grade_id": "cell-b11254d1a270988e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Codio Activity 18.4: Bag of Words: Count Vectorization\n",
    "\n",
    "**Expected Time = 60 minutes**\n",
    "\n",
    "**Total Points = 25**\n",
    "\n",
    "In this activity you will use the scikit-learn vectorization tool `CountVectorizer` to create a bag of words representation of text in a DataFrame.  You will explore how different parameter settings affect the performance of a `LogisticRegression` estimator on a binary classification problem.\n",
    "\n",
    "- [Problem 1](#-Problem-1)\n",
    "- [Problem 2](#-Problem-2)\n",
    "- [Problem 3](#-Problem-3)\n",
    "- [Problem 4](#-Problem-4)\n",
    "- [Problem 5](#-Problem-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fec13090b5f3425a619c337fea6a3d09",
     "grade": false,
     "grade_id": "cell-580b2c525505e59a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### The Data\n",
    "\n",
    "Below, the data from kaggle is again loaded.  Now, we join the \"sad\" and \"happy\" sentiments which will form the target of our classification models.  The data is also split and named appropriately below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_df = pd.read_csv(\"data/Emotion(happy).csv\")\n",
    "sad_df = pd.read_csv(\"data/Emotion(sad).csv.zip\", compression=\"zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat([happy_df, sad_df]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = full_df.drop(\"sentiment\", axis=1)\n",
    "y = full_df[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X[\"content\"], y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1287    ['You Hurt Me But I Still Love You.', 'True Lo...\n",
       "1112    Sorry isn’t always enough. Sometimes you actua...\n",
       "823     Sometimes two people have to fall apart to rea...\n",
       "651     True love isn’t love at first sight but love a...\n",
       "1101    i am scared of getting too close to anyone bec...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6b0f3d10df8766ebd65ffc73072f404d",
     "grade": false,
     "grade_id": "cell-1af9c3aaf69f904f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 1\n",
    "\n",
    "#### Using the `CountVectorizer`\n",
    "\n",
    "**5 Points**\n",
    "\n",
    "To create a bag of words representation of your text data, create an instance of the `CountVectorizer` as `cvect` below.  Leave all the default settings, and assign the transformed version of the text to `dtm`.  Note that because the vectorizer will return a `scipy.sparse` array, to view the contents of the resulting document term matrix the `toarray()` function is used together with the `.get_feature_names()` function to retrieve the fitted vocabulary.\n",
    "\n",
    "Hint: Make sure to transform X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf6c9720d2a66f85206ba601dc28737f",
     "grade": false,
     "grade_id": "cell-993194297edf95fd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_0</th>\n",
       "      <th>100</th>\n",
       "      <th>123whatsappstatus</th>\n",
       "      <th>204</th>\n",
       "      <th>30</th>\n",
       "      <th>404</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>55</th>\n",
       "      <th>805</th>\n",
       "      <th>...</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yet</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "      <th>your</th>\n",
       "      <th>yours</th>\n",
       "      <th>yourself</th>\n",
       "      <th>yous</th>\n",
       "      <th>yuh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1832 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0_0  100  123whatsappstatus  204  30  404  44  45  55  805  ...  yes  \\\n",
       "0    0    0                  0    0   0    0   0   0   0    0  ...    2   \n",
       "1    0    0                  0    0   0    0   0   0   0    0  ...    0   \n",
       "2    0    0                  0    0   0    0   0   0   0    0  ...    0   \n",
       "3    0    0                  0    0   0    0   0   0   0    0  ...    0   \n",
       "4    0    0                  0    0   0    0   0   0   0    0  ...    0   \n",
       "\n",
       "   yesterday  yet  you  young  your  yours  yourself  yous  yuh  \n",
       "0          0    1  112      0    13      0         2     0    0  \n",
       "1          0    0    1      0     0      0         0     0    0  \n",
       "2          0    0    0      0     0      0         0     0    0  \n",
       "3          0    0    0      0     0      0         0     0    0  \n",
       "4          0    0    0      0     0      0         0     0    0  \n",
       "\n",
       "[5 rows x 1832 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### GRADED\n",
    "cvect = CountVectorizer()\n",
    "dtm = cvect.fit_transform(X_train, y_train)\n",
    "\n",
    "### ANSWER CHECK\n",
    "pd.DataFrame(dtm.toarray(), columns=cvect.get_feature_names_out()).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d5026f6c711382b93c3023225221aa87",
     "grade": false,
     "grade_id": "cell-748f6d75238662a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Problem 2\n",
    "\n",
    "#### Limiting words with the `CountVectorizer`\n",
    "\n",
    "**5 Points**\n",
    "\n",
    "Now, to remove stopwords from the text before vectorizing create a new instance of the `CountVectorizer` and set `stop_words = 'english'` to remove the english language stop words using the same list as in our earlier assignment.  Fit and transform the training data and transform the test data as `X_train_vect_2` and `X_test_vect_2` below.\n",
    "\n",
    "Hint: Use `fit_transform` for the training data, and `transform` for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e7a176a89a462c375166de7afe0567f",
     "grade": false,
     "grade_id": "cell-3774a2265d6e5e27",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1007x1622 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 41589 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### GRADED\n",
    "cvect2 = CountVectorizer(stop_words=\"english\")\n",
    "X_train_vect_2 = cvect2.fit_transform(X_train, y_train)\n",
    "X_test_vect_2 = cvect2.transform(X_test)\n",
    "\n",
    "### ANSWER CHECK\n",
    "X_train_vect_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "caa80ba9e40f64102b66d819eac99763",
     "grade": false,
     "grade_id": "cell-08908731ede92487",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Problem 3\n",
    "\n",
    "#### Limiting words with stopwords and higher counts\n",
    "\n",
    "**5 Points**\n",
    "\n",
    "Now, remove stopwords using `stop_words = 'english'` and limit the features to the top 300 words based on counts using the `max_features` argument.  Fit and transform your data appropriately as `X_train_vect_3` and `X_test_vect_3` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71d81b27ffc8e89a34b16cfa48c2f805",
     "grade": false,
     "grade_id": "cell-fc155b6ae942f747",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1007x300 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 33225 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### GRADED\n",
    "cvect3 = CountVectorizer(stop_words=\"english\", max_features=300)\n",
    "X_train_vect_3 = cvect3.fit_transform(X_train, y_train)\n",
    "X_test_vect_3 = cvect3.transform(X_test)\n",
    "\n",
    "\n",
    "### ANSWER CHECK\n",
    "X_train_vect_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d531667d77e87b58f7603c5d77c0de16",
     "grade": false,
     "grade_id": "cell-52dbe3ec03ade066",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 4\n",
    "\n",
    "#### Using the text with `LogisticRegression`\n",
    "\n",
    "**5 Points**\n",
    "\n",
    "Create a `Pipeline` object named `vect_pipe_1` below that has steps named `cvect` and `lgr`, using both a default `CountVectorizer` transformer and `LogisticRegression` estimator. Fit this on the training data and evaluate it on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2b1d5888129599771a348635820333c",
     "grade": false,
     "grade_id": "cell-0ced72c8a300928f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cvect': CountVectorizer(), 'lgr': LogisticRegression()}, 0.8273809523809523]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### GRADED\n",
    "vect_pipe_1 = Pipeline(\n",
    "    [\n",
    "        (\"cvect\", CountVectorizer()),\n",
    "        (\"lgr\", LogisticRegression()),\n",
    "    ]\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "test_acc = vect_pipe_1.score(X_test, y_test)\n",
    "\n",
    "### ANSWER CHECK\n",
    "[\n",
    "    vect_pipe_1.named_steps,\n",
    "    test_acc,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba830e8717691f6547bd71b9c9c88081",
     "grade": false,
     "grade_id": "cell-67c7fd85b923ac68",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 5\n",
    "\n",
    "#### Pipeline and Grid Search\n",
    "\n",
    "**5 Points**\n",
    "\n",
    "Finally, to abstract this work into a single step you can create a `Pipeline` with named steps `cvect` and `lgr` below that vectorize and model the data.  Then, use the parameter grid to perform a grid search for the ideal parameters to represent the text and build a classification model. \n",
    "\n",
    "Hint: Use vect_pipe_1 from problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"cvect__max_features\": [100, 500, 1000, 2000],\n",
    "    \"cvect__stop_words\": [\"english\", None],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5c26d73d370648dd664d3dfddabdac9",
     "grade": false,
     "grade_id": "cell-e57872e309a8659f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvect__max_features': 2000, 'cvect__stop_words': None}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### GRADED\n",
    "grid = GridSearchCV(estimator=vect_pipe_1, param_grid=params).fit(X_train, y_train)\n",
    "test_acc = grid.best_estimator_.score(X_test, y_test)\n",
    "\n",
    "### ANSWER CHECK\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.971201588877855, 0.8273809523809523]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    grid.score(X_train, y_train),\n",
    "    grid.score(X_test, y_test),\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
